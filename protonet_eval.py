# -*- coding: utf-8 -*-
"""protonet_eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/naflanawas/breath_aac/blob/main_dev/notebooks/protonet_eval.ipynb

PERSONALISATION
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!rm -rf breath_aac
!git clone https://github.com/naflanawas/breath_aac.git
# %cd breath_aac
!git checkout main_dev

from google.colab import drive
drive.mount('/content/drive')

#!unzip -q "/content/drive/MyDrive/FYP_2025/Implementation/models.zip" -d /content/breath_aac/
#!ls models | head

!unzip -q "/content/drive/MyDrive/fyp/models.zip" -d /content/breath_aac/
!unzip -q "/content/drive/MyDrive/fyp/features.zip" -d /content/breath_aac/
!ls models | head
!ls features | head

import pandas as pd

SPLIT_CSV = "manifests/split_2c_subjectwise.csv"
K = 5

df = pd.read_csv(SPLIT_CSV)
test_df = df[df["split"] == "test"].copy()

# count samples per (subject, label)
counts = test_df.groupby(["subject_id", "label"]).size().unstack(fill_value=0)

# subjects that don't have enough for BOTH classes
bad = counts[(counts.min(axis=1) < K)]

print("Total test subjects:", counts.shape[0])
print(f"Subjects with < {K} samples in at least one class:", bad.shape[0])

# show a few
display(bad.head(20))

import numpy as np
import pandas as pd
import torch

from sklearn.metrics import accuracy_score, f1_score
import matplotlib.pyplot as plt

from src.train.train_ms_tcn_2c import MSTCN, pick_device
from src.features.protonet import compute_prototypes, prototypical_predict

SPLIT_CSV = "manifests/split_2c_subjectwise.csv"
CKPT = "models/ms_tcn_colab_1024.pt"

MAX_LEN = 1024
WINDOW_STRIDE = 512
K = 5
SEED = 7

df = pd.read_csv(SPLIT_CSV)

classes = sorted(df[df["split"] == "train"]["label"].unique())
c2i = {c:i for i,c in enumerate(classes)}
i2c = {i:c for c,i in c2i.items()}

print("Classes:", classes)

device = pick_device()
print("Device:", device)

model = MSTCN(in_ch=3, n_classes=len(classes)).to(device)
model.load_state_dict(torch.load(CKPT, map_location=device), strict=False)
model.eval()

for p in model.parameters():
    p.requires_grad = False

print("Loaded checkpoint:", CKPT)

def split_into_windows(x, win_len=1024, stride=512):
    """
    x: [3, 64, T]
    returns list of [3, 64, win_len]
    """
    _, _, T = x.shape
    windows = []

    if T < win_len:
        pad = np.zeros((3, 64, win_len - T), dtype=x.dtype)
        windows.append(np.concatenate([x, pad], axis=-1))
        return windows

    for start in range(0, T - win_len + 1, stride):
        windows.append(x[:, :, start:start + win_len])

    return windows

def load_npy_as_windows(path, win_len=1024, stride=512):
    x = np.load(path)  # [3,64,T]

    # CMVN (same as training)
    mean = x.mean(axis=(1,2), keepdims=True)
    std  = x.std(axis=(1,2), keepdims=True) + 1e-8
    x = (x - mean) / std

    windows = split_into_windows(x, win_len, stride)
    return [torch.from_numpy(w).float() for w in windows]

def embed_windows(windows_list):
    if len(windows_list) == 0:
        return None
    X = torch.stack(windows_list).to(device)
    with torch.no_grad():
        emb = model(X, return_embedding=True)
    return emb.cpu()

test_df = df[df["split"] == "test"].copy()
subjects = sorted(test_df["subject_id"].unique())

print("Number of test subjects:", len(subjects))

"""PROTONET IMPL

"""

WINDOW_LEN = 1024
WINDOW_STRIDE = 512

rng = np.random.default_rng(SEED)

all_true_global, all_pred_global = [], []
all_true_proto,  all_pred_proto  = [], []
per_subject_rows = []
explain_logs = []

for sid in subjects:
    sdf = test_df[test_df["subject_id"] == sid]

    # collect windows per class
    windows_long, windows_short = [], []

    for _, row in sdf.iterrows():
        windows = load_npy_as_windows(
            row["filepath"],
            win_len=WINDOW_LEN,
            stride=WINDOW_STRIDE
        )
        if row["label"] == "long":
            windows_long.extend(windows)
        else:
            windows_short.extend(windows)

    # dynamic K (WINDOW level)
    k_subj = min(K, len(windows_long), len(windows_short))
    if k_subj < 1:
        continue

    # sample SUPPORT windows
    sup_long  = rng.choice(len(windows_long),  k_subj, replace=False)
    sup_short = rng.choice(len(windows_short), k_subj, replace=False)

    support_windows = (
        [windows_long[i] for i in sup_long] +
        [windows_short[i] for i in sup_short]
    )
    support_labels = torch.tensor(
        [c2i["long"]]*k_subj + [c2i["short"]]*k_subj
    )

    # QUERY windows = rest
    qry_long  = [w for i,w in enumerate(windows_long)  if i not in sup_long]
    qry_short = [w for i,w in enumerate(windows_short) if i not in sup_short]

    if len(qry_long) + len(qry_short) == 0:
        continue

    query_windows = qry_long + qry_short
    query_labels = torch.tensor(
        [c2i["long"]]*len(qry_long) + [c2i["short"]]*len(qry_short)
    )

    # ---- embeddings ----
    sup_emb = embed_windows(support_windows)
    qry_emb = embed_windows(query_windows)

    # ---- ProtoNet ----
    prototypes = compute_prototypes(sup_emb, support_labels, n_classes=2)
    #proto_preds = prototypical_predict(qry_emb, prototypes)

    prototypes = compute_prototypes(sup_emb, support_labels, n_classes=2)

    proto_preds = []

    for i in range(qry_emb.shape[0]):
        z = qry_emb[i]  # one query embedding

        # distances to prototypes
        dists = torch.cdist(z.unsqueeze(0), prototypes).squeeze(0)

        # sort distances
        sorted_dists, sorted_idx = torch.sort(dists)

        d1 = sorted_dists[0].item()
        d2 = sorted_dists[1].item()
        confidence_margin = d2 - d1

        pred = sorted_idx[0].item()
        proto_preds.append(pred)

        # ðŸ” LOG explainability
        explain_logs.append({
            "subject_id": sid,
            "true_label": int(query_labels[i].item()),
            "pred_label": int(pred),
            "d1": d1,
            "d2": d2,
            "confidence_margin": confidence_margin,
            "k_support": k_subj
        })

    proto_preds = torch.tensor(proto_preds)

    # ---- Global ----
    Xq = torch.stack(query_windows).to(device)
    with torch.no_grad():
        global_preds = model(Xq).argmax(dim=1).cpu()

    # collect
    all_true_proto.extend(query_labels.numpy())
    all_pred_proto.extend(proto_preds.numpy())
    all_true_global.extend(query_labels.numpy())
    all_pred_global.extend(global_preds.numpy())

    per_subject_rows.append([
        sid,
        k_subj,
        len(query_labels),
        accuracy_score(query_labels, global_preds),
        f1_score(query_labels, global_preds, average="macro"),
        accuracy_score(query_labels, proto_preds),
        f1_score(query_labels, proto_preds, average="macro"),
    ])

print("Done subjects:", len(per_subject_rows))

exp_df = pd.DataFrame(explain_logs)
exp_df.to_csv("protonet_explainability.csv", index=False)

print("Saved explainability log:", exp_df.shape)

"""Sanity check for 51 subjects"""

import pandas as pd
import numpy as np

WINDOW_LEN = 1024
WINDOW_STRIDE = 512

def count_windows(path, win_len=1024, stride=512):
    x = np.load(path)  # shape [3, 64, T]
    T = x.shape[-1]
    if T < win_len:
        return 0
    return 1 + (T - win_len) // stride


df = pd.read_csv("manifests/split_2c_subjectwise.csv")
test_df = df[df["split"] == "test"].copy()

rows = []

for sid in sorted(test_df["subject_id"].unique()):
    sdf = test_df[test_df["subject_id"] == sid]

    short_w = 0
    long_w = 0

    for _, row in sdf.iterrows():
        n_w = count_windows(row["filepath"], WINDOW_LEN, WINDOW_STRIDE)
        if row["label"] == "short":
            short_w += n_w
        else:
            long_w += n_w

    rows.append({
        "subject_id": sid,
        "short_windows": short_w,
        "long_windows": long_w,
        "min_windows": min(short_w, long_w)
    })

window_df = pd.DataFrame(rows)

print("Total test subjects:", len(window_df))
print("Subjects with >=1 window per class:", (window_df["min_windows"] >= 1).sum())
print("Subjects with 0 usable windows:", (window_df["min_windows"] == 0).sum())

window_df.head()

window_df.describe()

window_df["min_windows"].value_counts().sort_index()

import matplotlib.pyplot as plt

plt.figure(figsize=(8,4))
plt.hist(window_df["short_windows"], bins=30, alpha=0.6, label="short")
plt.hist(window_df["long_windows"], bins=30, alpha=0.6, label="long")
plt.yscale("log")
plt.xlabel("Number of windows")
plt.ylabel("Number of subjects (log scale)")
plt.title("Window distribution per class (test subjects)")
plt.legend()
plt.show()

res = pd.DataFrame(
    per_subject_rows,
    columns=["subject_id", "k_used", "n_query", "global_acc", "global_f1", "proto_acc", "proto_f1"]
)

display(res.head())

print("\n=== GLOBAL MODEL ===")
print("Acc:", accuracy_score(all_true_global, all_pred_global))
print("F1 :", f1_score(all_true_global, all_pred_global, average="macro"))

print("\n=== PROTONET (PERSONALISED) ===")
print("Acc:", accuracy_score(all_true_proto, all_pred_proto))
print("F1 :", f1_score(all_true_proto, all_pred_proto, average="macro"))

plt.figure(figsize=(8,4))
plt.plot(res["proto_f1"] - res["global_f1"], marker="o", linestyle="none")
plt.axhline(0, color="red")
plt.xlabel("Subject index")
plt.ylabel("Î” F1 (Proto âˆ’ Global)")
plt.title("Per-subject improvement after personalisation")
plt.grid(True)
plt.show()

import numpy as np

res["delta_f1"]  = res["proto_f1"] - res["global_f1"]
res["delta_acc"] = res["proto_acc"] - res["global_acc"]

print("Subjects evaluated:", len(res))

print("\n=== Personalisation impact (macro-F1) ===")
print("Mean Î”F1:", res["delta_f1"].mean())
print("Median Î”F1:", res["delta_f1"].median())
print("Improved subjects (%):", (res["delta_f1"] > 0).mean() * 100)
print("No change subjects (%):", (res["delta_f1"] == 0).mean() * 100)
print("Worse subjects (%):", (res["delta_f1"] < 0).mean() * 100)

print("\nTop 10 improvements:")
display(res.sort_values("delta_f1", ascending=False).head(10))

print("\nTop 10 drops:")
display(res.sort_values("delta_f1", ascending=True).head(10))

from sklearn.metrics import recall_score

print("Global macro recall:", recall_score(all_true_global, all_pred_global, average="macro"))
print("Proto  macro recall:", recall_score(all_true_proto,  all_pred_proto,  average="macro"))

import matplotlib.pyplot as plt

plt.figure()
exp_df["confidence_margin"].hist(bins=30)
plt.title("ProtoNet Confidence Margin Distribution")
plt.xlabel("d2 - d1")
plt.ylabel("Count")
plt.show()

exp_df.boxplot(column="confidence_margin", by="pred_label")
plt.title("Confidence by Predicted Class")
plt.suptitle("")
plt.show()

torch.save(model.state_dict(), "protonet_personalised.pth")
print("Model saved as protonet_personalised.pth")